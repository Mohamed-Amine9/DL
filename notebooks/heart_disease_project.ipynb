{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6. MODEL BUILDING & TRAINING (Person B - Phase 5)\n",
        "# ============================================================\n",
        "\n",
        "## Phase 5: Data Splitting & Model Architecture Design\n",
        "\n",
        "**Person B's Tasks:**\n",
        "- Load preprocessed data from Person A\n",
        "- Split data into train/validation/test sets\n",
        "- Design neural network architecture\n",
        "- Compile model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.1: Load Preprocessed Data from Person A\n",
        "# ============================================================\n",
        "\n",
        "# Import Person A's preprocessing function\n",
        "# (This function should be defined in Section 5 above)\n",
        "# If not, uncomment and run the function definition from Section 5 first\n",
        "\n",
        "# Load preprocessed data\n",
        "X, y, feature_names, scaler = load_and_preprocess()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADED FROM PERSON A\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ“ Data loaded successfully!\")\n",
        "print(f\"  - Features shape: {X.shape}\")\n",
        "print(f\"  - Target shape: {y.shape}\")\n",
        "print(f\"  - Number of features: {len(feature_names)}\")\n",
        "print(f\"  - Feature names: {feature_names[:5]}... (showing first 5)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.2: Split Data into Train/Validation/Test Sets\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split into train (70%) and temp (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.3, \n",
        "    random_state=42, \n",
        "    stratify=y  # Maintain class balance\n",
        ")\n",
        "\n",
        "# Split temp into validation (15%) and test (15%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5,  # 0.5 of 0.3 = 0.15 (15%)\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA SPLITTING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nâœ“ Data split completed!\")\n",
        "print(f\"  - Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verify class distribution\n",
        "print(f\"\\nðŸ“Š Class Distribution:\")\n",
        "print(f\"  Training - Class 0: {(y_train == 0).sum()}, Class 1: {(y_train == 1).sum()}\")\n",
        "print(f\"  Validation - Class 0: {(y_val == 0).sum()}, Class 1: {(y_val == 1).sum()}\")\n",
        "print(f\"  Test - Class 0: {(y_test == 0).sum()}, Class 1: {(y_test == 1).sum()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.3: Design Neural Network Architecture\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define model architecture\n",
        "model = keras.Sequential([\n",
        "    # Input layer\n",
        "    layers.Dense(64, activation='relu', input_shape=(15,), name='input_layer'),\n",
        "    layers.Dropout(0.3, name='dropout_1'),\n",
        "    \n",
        "    # Hidden layer 1\n",
        "    layers.Dense(32, activation='relu', name='hidden_layer_1'),\n",
        "    layers.Dropout(0.3, name='dropout_2'),\n",
        "    \n",
        "    # Hidden layer 2\n",
        "    layers.Dense(16, activation='relu', name='hidden_layer_2'),\n",
        "    layers.Dropout(0.2, name='dropout_3'),\n",
        "    \n",
        "    # Output layer (binary classification)\n",
        "    layers.Dense(1, activation='sigmoid', name='output_layer')\n",
        "])\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL ARCHITECTURE CREATED\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ“ Model architecture created!\")\n",
        "print(\"\\nðŸ“‹ Model Summary:\")\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.4: Compile Model\n",
        "# ============================================================\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',  # Binary classification\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL COMPILED\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ“ Model compiled successfully!\")\n",
        "print(\"  - Optimizer: Adam (lr=0.001)\")\n",
        "print(\"  - Loss: binary_crossentropy\")\n",
        "print(\"  - Metrics: accuracy, precision, recall\")\n",
        "print(\"\\nâœ… Phase 5 Complete - Model ready for training!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 7. MODEL TRAINING (Person B - Phase 6)\n",
        "# ============================================================\n",
        "\n",
        "## Phase 6: Model Training & Early Stopping\n",
        "\n",
        "**Person B's Tasks:**\n",
        "- Set up callbacks (early stopping, model checkpointing)\n",
        "- Train the model\n",
        "- Visualize training history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6.1: Set Up Callbacks\n",
        "# ============================================================\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    # Early stopping: stop if validation loss doesn't improve\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,  # Wait 15 epochs\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    ),\n",
        "    \n",
        "    # Save best model\n",
        "    ModelCheckpoint(\n",
        "        filepath='../models/heart_failure_model_best.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    ),\n",
        "    \n",
        "    # Reduce learning rate if plateau\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CALLBACKS CONFIGURED\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ“ Callbacks configured:\")\n",
        "print(\"  - EarlyStopping: patience=15, monitor='val_loss'\")\n",
        "print(\"  - ModelCheckpoint: save best model to '../models/heart_failure_model_best.h5'\")\n",
        "print(\"  - ReduceLROnPlateau: factor=0.5, patience=5\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6.2: Train the Model\n",
        "# ============================================================\n",
        "\n",
        "# Train the model\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(\"Starting training...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,  # Maximum epochs\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ“ Training completed!\")\n",
        "print(f\"  - Total epochs trained: {len(history.history['loss'])}\")\n",
        "print(f\"  - Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
        "print(f\"  - Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
        "print(f\"  - Model saved to: '../models/heart_failure_model_best.h5'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6.3: Visualize Training History\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision\n",
        "axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
        "axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)\n",
        "axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Precision')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Recall\n",
        "axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
        "axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2)\n",
        "axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Recall')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING HISTORY VISUALIZED\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ“ Training history plots displayed\")\n",
        "print(\"\\nâœ… Phase 6 Complete - Model trained and ready for evaluation!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 8. HYPERPARAMETER TUNING & MODEL OPTIMIZATION (Person B - Phase 7)\n",
        "# ============================================================\n",
        "\n",
        "## Phase 7: Hyperparameter Tuning & Model Optimization\n",
        "\n",
        "**Person B's Tasks:**\n",
        "- Evaluate initial model on test set\n",
        "- Perform hyperparameter tuning\n",
        "- Train optimized model\n",
        "- Evaluate final model\n",
        "- Save final model for Person C\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7.1: Evaluate Initial Model\n",
        "# ============================================================\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load best model from Phase 6\n",
        "best_model = load_model('../models/heart_failure_model_best.h5')\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATING INITIAL MODEL (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_loss, test_accuracy, test_precision, test_recall = best_model.evaluate(\n",
        "    X_test, y_test, verbose=0\n",
        ")\n",
        "\n",
        "print(\"ðŸ“Š Initial Model Performance (Test Set):\")\n",
        "print(f\"  - Loss: {test_loss:.4f}\")\n",
        "print(f\"  - Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"  - Precision: {test_precision:.4f}\")\n",
        "print(f\"  - Recall: {test_recall:.4f}\")\n",
        "\n",
        "# Calculate F1 score\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "print(f\"  - F1 Score: {f1_score:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_proba = best_model.predict(X_test, verbose=0)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Disease', 'Disease']))\n",
        "\n",
        "print(\"\\nâœ… Initial model evaluation complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7.2: Hyperparameter Tuning\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*60)\n",
        "print(\"Testing different learning rates...\")\n",
        "print(\"(This may take a few minutes)\\n\")\n",
        "\n",
        "# Try different learning rates\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "tuning_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nðŸ”§ Testing learning rate: {lr}\")\n",
        "    \n",
        "    # Create new model with same architecture\n",
        "    model_tune = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(15,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model_tune.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Train with fewer epochs for tuning\n",
        "    early_stop_tune = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    history_tune = model_tune.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop_tune],\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_acc = model_tune.evaluate(X_val, y_val, verbose=0)\n",
        "    tuning_results.append({\n",
        "        'lr': lr,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'epochs': len(history_tune.history['loss'])\n",
        "    })\n",
        "    print(f\"  âœ“ Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Epochs: {len(history_tune.history['loss'])}\")\n",
        "\n",
        "# Find best learning rate\n",
        "best_lr_result = min(tuning_results, key=lambda x: x['val_loss'])\n",
        "best_lr = best_lr_result['lr']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“Š Results Summary:\")\n",
        "for result in tuning_results:\n",
        "    marker = \" â­ BEST\" if result['lr'] == best_lr else \"\"\n",
        "    print(f\"  LR {result['lr']:6.4f}: Val Loss={result['val_loss']:.4f}, Val Acc={result['val_acc']:.4f}{marker}\")\n",
        "\n",
        "print(f\"\\nâœ“ Best learning rate: {best_lr} (Val Loss: {best_lr_result['val_loss']:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7.3: Train Final Optimized Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING FINAL OPTIMIZED MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Using best learning rate: {best_lr}\")\n",
        "print(\"Training with full epochs...\\n\")\n",
        "\n",
        "# Create optimized model with best hyperparameters\n",
        "final_model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(15,)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "final_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=best_lr),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "\n",
        "# Train final model with full callbacks\n",
        "final_history = final_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ“ Final model trained!\")\n",
        "print(f\"  - Total epochs: {len(final_history.history['loss'])}\")\n",
        "print(f\"  - Best validation loss: {min(final_history.history['val_loss']):.4f}\")\n",
        "print(f\"  - Best validation accuracy: {max(final_history.history['val_accuracy']):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7.4: Evaluate Final Model on Test Set\n",
        "# ============================================================\n",
        "\n",
        "# Load best final model\n",
        "final_best_model = load_model('../models/heart_failure_model_best.h5')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATING FINAL MODEL (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = final_best_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"ðŸ“Š Final Model Performance (Test Set):\")\n",
        "print(f\"  - Loss: {test_results[0]:.4f}\")\n",
        "print(f\"  - Accuracy: {test_results[1]:.4f} ({test_results[1]*100:.2f}%)\")\n",
        "print(f\"  - Precision: {test_results[2]:.4f}\")\n",
        "print(f\"  - Recall: {test_results[3]:.4f}\")\n",
        "\n",
        "# Calculate F1 score\n",
        "final_f1 = 2 * (test_results[2] * test_results[3]) / (test_results[2] + test_results[3])\n",
        "print(f\"  - F1 Score: {final_f1:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_proba_final = final_best_model.predict(X_test, verbose=0)\n",
        "y_pred_final = (y_pred_proba_final > 0.5).astype(int).flatten()\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
        "print(cm_final)\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=['No Disease', 'Disease']))\n",
        "\n",
        "# Compare with initial model\n",
        "print(\"\\nðŸ“Š Performance Comparison:\")\n",
        "print(f\"  Initial Model Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"  Final Model Accuracy:   {test_results[1]:.4f} ({test_results[1]*100:.2f}%)\")\n",
        "improvement = test_results[1] - test_accuracy\n",
        "if improvement > 0:\n",
        "    print(f\"  âœ“ Improvement: +{improvement:.4f} (+{improvement*100:.2f}%)\")\n",
        "elif improvement < 0:\n",
        "    print(f\"  âš  Change: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
        "else:\n",
        "    print(f\"  â†’ No change\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7.5: Save Final Model\n",
        "# ============================================================\n",
        "\n",
        "import pickle\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING FINAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save final model\n",
        "final_best_model.save('../models/heart_failure_model.h5')\n",
        "print(\"âœ“ Final model saved to '../models/heart_failure_model.h5'\")\n",
        "\n",
        "# Also save scaler (needed for predictions)\n",
        "with open('../models/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"âœ“ Scaler saved to '../models/scaler.pkl'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… PHASE 7 COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ“ Model optimized and saved\")\n",
        "print(\"âœ“ Ready for Person C (Evaluation & Reporting)\")\n",
        "print(\"\\nðŸ“¦ Deliverables:\")\n",
        "print(\"  - models/heart_failure_model.h5 (final model)\")\n",
        "print(\"  - models/scaler.pkl (preprocessing scaler)\")\n",
        "print(\"  - Test set performance metrics\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
